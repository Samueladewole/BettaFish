## Topic Classification (BERT Chinese Base)

> **Note**: This is an English translation of the original Chinese documentation. The original project [BettaFish](https://github.com/666ghj/BettaFish) was created by [666ghj](https://github.com/666ghj). This translation was contributed to help non-Chinese speakers understand and use this project.

This directory provides a Chinese topic classification implementation using `google-bert/bert-base-chinese`:
- Automatically handles three-stage loading logic: local/cache/remote;
- `train.py` for fine-tuning training; `predict.py` for single or interactive prediction;
- All models and weights are saved to the `model/` directory in this location.

Reference model card: [google-bert/bert-base-chinese](https://huggingface.co/google-bert/bert-base-chinese)

### Dataset Highlights

- Approximately **4.1 million** pre-filtered high-quality questions and answers;
- Each question corresponds to a "Topic", covering **approximately 28,000** diverse topics;
- Filtered from **14 million** original Q&A pairs, retaining answers with at least **3 upvotes** to ensure content quality and interest;
- Besides questions, topics, and one or more answers, each answer also has upvote counts, answer ID, and answerer tags;
- After data cleaning and deduplication, split into three parts: training set approximately **4.12 million**, validation/test sets as needed (adjustable).

> For actual training, please refer to the CSV files in `dataset/`; the script will automatically recognize common column names or allow explicit specification via command-line arguments.

### Directory Structure

```
BertTopicDetection_Finetuned/
  ├─ dataset/                   # Data placed here
  ├─ model/                     # Generated by training; also caches base BERT
  ├─ train.py
  ├─ predict.py
  └─ README.md
```

### Environment

```
pip install torch transformers scikit-learn pandas
```

Or use your existing Conda environment.

### Data Format

CSV must contain at least a text column and a label column; the script will attempt to auto-detect:
- Text column candidates: `text`/`content`/`sentence`/`title`/`desc`/`question`
- Label column candidates: `label`/`labels`/`category`/`topic`/`class`

For explicit specification, use `--text_col` and `--label_col`.

### Training

```
python train.py \
  --train_file ./dataset/web_text_zh_train.csv \
  --valid_file ./dataset/web_text_zh_valid.csv \
  --text_col auto \
  --label_col auto \
  --model_root ./model \
  --save_subdir bert-chinese-classifier \
  --num_epochs 10 --batch_size 16 --learning_rate 2e-5 --fp16
```

Key points:
- First run will check `model/bert-base-chinese`; if not found, tries local cache, then auto-downloads and saves;
- Training process evaluates and saves by step (default every 1/4 epoch), keeps up to 5 recent checkpoints (adjustable via `SAVE_TOTAL_LIMIT` environment variable);
- Supports early stopping (default patience 5 evaluations), and automatically rolls back to best model when evaluation/save strategy is consistent;
- Tokenizer, weights, and `label_map.json` are saved to `model/bert-chinese-classifier/`.

### Optional Chinese Base Models (Interactive Selection Before Training)

Default base: `google-bert/bert-base-chinese`. When starting training, if the terminal is interactive, the program will prompt to select from the following options (or enter any Hugging Face model ID):

1) `google-bert/bert-base-chinese`
2) `hfl/chinese-roberta-wwm-ext-large`
3) `hfl/chinese-macbert-large`
4) `IDEA-CCNL/Erlangshen-DeBERTa-v2-710M-Chinese`
5) `IDEA-CCNL/Erlangshen-DeBERTa-v3-Base-Chinese`
6) `Langboat/mengzi-bert-base`
7) `BAAI/bge-base-zh` (more suitable for retrieval/contrastive learning paradigms)
8) `nghuyong/ernie-3.0-base-zh`

Notes:
- In non-interactive environments (like scheduling systems) or when `NON_INTERACTIVE=1` is set, the model specified by command-line argument `--pretrained_name` is used directly (default `google-bert/bert-base-chinese`).
- After selection, the base model will be downloaded/cached to the `model/` directory for unified management.

### Prediction

Single prediction:
```
python predict.py --text "What topic is this Weibo post about?" --model_root ./model --finetuned_subdir bert-chinese-classifier
```

Interactive:
```
python predict.py --interactive --model_root ./model --finetuned_subdir bert-chinese-classifier
```

Example output:
```
Prediction: Sports-Football (Confidence: 0.9412)
```

### Notes

- Both training and prediction include basic Chinese text cleaning.
- Label set is based on the training set; the script automatically generates and saves `label_map.json`.

### Training Strategy (Brief)

- Base: `google-bert/bert-base-chinese`; classification head dimension = number of unique training set labels.
- Learning rate and regularization: `lr=2e-5`, `weight_decay=0.01`, can be fine-tuned to `1e-5~3e-5` for large datasets.
- Sequence length and batch size: `max_length=128`, `batch_size=16`; if severe truncation occurs, can increase to 256 (higher cost).
- Warmup: Uses `warmup_ratio=0.1` if environment supports; otherwise falls back to `warmup_steps=0`.
- Evaluation/saving: Steps calculated from `--eval_fraction` (default 0.25), `save_total_limit=5` limits disk usage.
- Early stopping: Monitors weighted F1 (higher is better), default patience 5, improvement threshold 0.0.
- Single GPU stable operation: Uses only one GPU by default, can specify via `--gpu`; script cleans distributed environment variables.

### Author's Notes (Regarding Ultra-large Scale Multi-classification)

- When topic categories reach tens of thousands, using a single linear classification head (large softmax) after the encoder is often limited: long-tail categories are hard to learn, semantics are sparse, new topics cannot be incrementally adapted, and frequent retraining is needed after deployment.
- Improvement approaches (recommended priority):
  - Retrieval/dual-tower paradigm (text vs. topic name/description contrastive learning) + nearest neighbor retrieval + small head re-ranking, naturally supports incremental class expansion and fast updates;
  - Hierarchical classification (coarse first, then fine), significantly reduces single-head difficulty and computation;
  - Text-label joint modeling (using label descriptions), improves transferability for synonymous topics;
  - Training details: class-balanced/focal/label smoothing, sampled softmax, contrastive pre-training, etc.
- Important statement: The "static classification head fine-tuning" in this directory is only provided as an alternative and learning reference. For English/multilingual micro-short text scenarios where topics change rapidly, traditional static classifiers struggle to cover in time. Our main work focuses on `TopicGPT` and other generative/self-supervised topic discovery and dynamic taxonomy construction; this implementation aims to provide a runnable baseline and engineering example.
